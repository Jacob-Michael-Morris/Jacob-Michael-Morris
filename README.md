
<!--
**Jacob-Michael-Morris/Jacob-Michael-Morris** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ğŸ”­ Iâ€™m currently working on ...
- ğŸŒ± Iâ€™m currently learning ...

-->

# ğŸ‘‹ Welcome to My GitHub!

ğŸ“« **Email:** [JacobMichaelMorris@gmail.com](mailto:JacobMichaelMorris@gmail.com)  
ğŸ”— **LinkedIn:** [linkedin.com/in/jacob-m-morris](https://www.linkedin.com/in/jacob-m-morris)  
ğŸ“ **Location:** Seattle, WA

---

## ğŸ§‘â€ğŸ’¼ Professional Summary

Hello! I'm **Jacob Morris**, a transitioning veteran with 5+ years of experience in:

- Technical data management  
- AI / Machine Learning  
- Software development  
- Software quality assurance

---

## ğŸš§ What I'm Currently Working On

| Project | Description |
|--------|-------------|
| [**Project One**](https://github.com/yourusername/project1) | A short description of this project, what it does, and why it matters. |
| [**Project Two**](https://github.com/yourusername/project2) | Another cool project youâ€™ve worked on. Keep it brief but engaging. |

---

## âœ… What I've Done in the Past

| Project | Description |
|--------|-------------|
| [**Applying Lightweight Fine-Tuning to a Foundation Model**](https://colab.research.google.com/drive/1wwhmrDoLZdVfIpo1l1RKV3vImWJBjkFt?usp=sharing) | In this project I evaluated, fine-tuned, and compared a pretrained transformer model using PEFT with LoRA. It begins by loading a pretrained distilbert model and the AG News dataset, tokenizing, and evaluating the base model on a small subset to establish baseline performance. Then LoRA is set up to target specific attention layers within distilbert, which allows lightweight fine-tuning by training only the adapter layers while freezing the rest of the model. The fine-tuned model is trained using Hugging Face and finally and evaluation is done on the same test subset which is compared to the baseline. This project highlights how LoRA enables efficient and effective fine-tuning with minimal resource usage while maintaining competitive performance. |
| [**Project Two**](https://github.com/yourusername/project2) | Another cool project youâ€™ve worked on. Keep it brief but engaging. |

---

## ğŸ› ï¸ Languages and Tools

<p align="center">
  <img src="https://skillicons.dev/icons?i=js,html,css,py,java,c,cpp,cs" alt="Languages" />
  <br><br>
  <img src="https://skillicons.dev/icons?i=react,git,mongodb,mysql" alt="Tools" />
</p>

**âš™ï¸ Other Skills:** AI, ML, LLMs

---

## ğŸ“¬ Contact

- ğŸ“§ [JacobMichaelMorris@gmail.com](mailto:JacobMichaelMorris@gmail.com)  
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/jacob-m-morris)
